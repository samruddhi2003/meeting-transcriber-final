Everybody's talking about artificial intelligence these days, AI.
Machine learning is another hot topic.
Are they the same thing, or are they different?
And if so, what are those differences?
And deep learning is another one that comes into play.
I actually did a video on these three,
artificial intelligence, machine learning, and deep learning,
and talked about where they fit.
And there were a lot of comments on that,
and I read those comments, and I'd like to address
some of the most frequently asked questions
so that we can clear up some of the myths and misconceptions
around this.
In addition, something else has happened
since that video was recorded.
And that is the absolute explosion of this area of generative AI.
Things like large language models and chatbots
have seemed to be taking over the world.
We see them everywhere, really interesting technology.
And then also things like deep fakes.
These are all within the realm of AI,
but how do they fit within each other?
How are they related to each other?
We're going to take a look at that in this video
and try to explain how all these technologies relate
and how we can use them.
First off, a little bit of a disclaimer.
I'm going to have to simplify some of these concepts
in order to not make this video last for a week.
So those of you that are really deep experts
in the field apologies in advance.
But we're going to try to make this simple,
and that will involve some generalizations.
First of all, to start with AI.
Artificial intelligence is basically trying to simulate
with a computer something that would match
or exceed human intelligence.
What is intelligence?
Well, it could be a lot of different things,
but generally we tend to think of it
as the ability to learn, to infer, and to reason,
things like that.
So that's what we're trying to do in the broad field of AI,
of artificial intelligence.
And if we look at a timeline of AI,
it really kind of started back around this time frame.
And in those days, it was very premature.
Most people had not even heard of it.
And it basically was a research project.
But I can tell you, as an undergrad, which for me
was back during these times, we were doing AI work.
In fact, we would use programming languages
like Lisp or ProLog.
And these kinds of things were kind of the predecessors
to what became later expert systems.
And this was a technology.
Again, some of these things existed previous,
but that's when it really hit kind of a critical mass
and became more popularized.
So expert systems of the 1980s, maybe in the 90s.
And again, we used technologies like this.
All of this was something that we did
before we ever touched in to the next topic
I'm going to talk about.
And that's the area of machine learning.
Machine learning is, as its name implies,
the machine is learning.
I don't have to program it.
I give it lots of information and it observes things.
So for instance, if I start doing this,
if I give you this and then ask you to predict
what's the next thing that's going to be there,
well, you might get it, you might not.
And you have very limited training data to base this on.
But if I gave you one of those and then ask you
what to predict would happen next,
well, you're probably going to say this.
And then you're going to say it's this.
And then you think you got it all figured out.
And then you see one of these.
And then all of a sudden, I give you one of those
and throw you a curveball.
So this, in fact, and then maybe it goes on like this.
So a machine learning algorithm is really good
at looking at patterns and discovering patterns
within data.
The more training data you can give it,
the more confident it can be in predicting.
So predictions are one of the things
that machine learning is particularly good at.
Another thing is spotting outliers like this
and saying, oh, that doesn't belong in,
it looks different than all the other stuff
because the sequence was broken.
So that's particularly useful in cybersecurity,
the area that I work in, because we're looking for outliers.
We're looking for users who are using the system
in ways that they shouldn't be or ways
that they don't typically do.
So this technology machine learning
is particularly useful for us.
And machine learning really came along
and became more popularized in this time frame,
in the 2010s.
And again, back when I was an undergrad riding my dinosaur
to class, we were doing this kind of stuff.
We never once talked about machine learning.
It might have existed, but it really
wasn't, hadn't hit the popular mindset yet.
But this technology has matured greatly
over the last few decades, and now
it becomes the basis of a lot we do going forward.
The next layer of our Venn diagram involves deep learning.
Well, it's deep learning in the sense that with deep learning,
we use these things called neural networks.
Neural networks are ways that in a computer,
we simulate and mimic the way the human brain works,
at least to the extent that we understand how the brain works.
And it's called deep because we have multiple layers
of those neural networks.
And the interesting thing about these
is they will simulate the way a brain operates
but I don't know if you've noticed,
but human brains can be a little bit unpredictable.
You put certain things in, you don't always
get the very same thing out.
And deep learning is the same way.
In some cases, we're not actually able to fully understand
why we get the results we do because there
are so many layers to the neural network.
It's a little bit hard to decompose
and figure out exactly what's in there.
But this has become a very important part
and a very important advancement.
That also reached some popularity during the 2010s
and as something that we use still today
as the basis for our next area of AI.
The most recent advancements in the field
of artificial intelligence all really are in this space,
the area of generative AI.
Now, I'm going to introduce a term
that you may not be familiar with.
It's the idea of foundation models.
Foundation models is where we get some of these kinds of things.
For instance, an example of a foundation model
would be a large language model, which
is where we take language and we model it.
And we make predictions in this technology
where if I see certain types of words,
then I can sort of predict what the next set of words will be.
I'm going to oversimplify here for the sake of simplicity.
But think about this as a little bit
like the autocomplete.
When you start typing something in,
and then it predicts what your next word will be,
except in this case with large language models,
they're not predicting the next word.
They're predicting the next sentence, the next paragraph,
the next entire document.
So there's a really an amazing exponential leap
in what these things are able to do.
And we call all of these technologies generative
because they are generating new content.
Some people have actually made the argument
that the generative AI isn't really
generative that these technologies are really just
regurgitating existing information
and putting it in different format.
Well, let me give you an analogy.
If you take music, for instance, then every note
has already been invented.
So in a sense, every song is just a recombination,
some other permutation of all the notes that already
exist already and just putting them in a different order.
Well, we don't say new music doesn't exist.
People are still composing and creating new songs
from the existing information.
I'm going to say gen AI is similar.
It's an analogy, so there'll be some imperfections in it,
but you get the general idea.
Actually, new content can be generated out of these.
And there are a lot of different forms that this can take.
Other types of models are audio models, video models,
and things like that.
Well, in fact, these we can use to create deepfakes.
And deepfakes are examples where we're able to take,
for instance, a person's voice and recreate that.
And then habits seem like the person said things
they never said.
Well, it's really useful in entertainment situations,
in parodies and things like that,
or if someone's losing their voice,
then you could capture their voice,
and then they'd be able to type
and you'd be able to hear it in their voice.
But there's also a lot of cases
where this stuff could be abused.
The chat bots, again, come from this space,
the deepfakes come from this space,
but they're all part of generative AI
and all part of these foundation models.
And this, again, is the area that has really caused all of us
to really pay attention to AI.
The possibilities of generating new content,
or in some cases, summarizing existing content,
and giving us something that is bite size and manageable.
This is what has gotten all of the attention.
This is where the chat bots and all of these things come in.
In the early days, AI's adoption started off pretty slowly.
Most people didn't even know it existed,
and if they did, it was something that always seemed like
it was about five to 10 years away.
But then machine learning, deep learning,
and things like that came along,
and we started seeing some uptick.
Then foundation models, Gen AI,
and the light came along,
and this stuff went straight to the moon.
These foundation models are what have changed the adoption curve,
and now you see AI being adopted everywhere.
And the thing for us to understand
is where this is, where it fits in,
and make sure that we can reap the benefits
from all of this technology.
If you like this video and want to see more like it,
please like and subscribe.
If you have any questions or want to share your thoughts
about this topic, please leave a comment below.
