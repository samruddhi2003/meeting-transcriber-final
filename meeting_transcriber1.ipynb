{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZAMIoyWDzIQOIStj8RwxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samruddhi2003/meeting-transcriber-final/blob/main/meeting_transcriber1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Install Dependencies**"
      ],
      "metadata": {
        "id": "L6X-txp42V7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-t5L3riw0zx",
        "outputId": "b7a4bf4b-3f6d-41a0-fea1-3b705063224f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-2zbkgb4c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-2zbkgb4c\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install transformers\n",
        "!pip install ffmpeg-python\n",
        "!pip install torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "h0kT4MSkyLol",
        "outputId": "27a0de99-848e-4609-e1ff-28dd73312897"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffc0933b-45aa-4dbd-98ad-eaea35c51fae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ffc0933b-45aa-4dbd-98ad-eaea35c51fae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-264872163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n"
      ],
      "metadata": {
        "id": "V4V4sVhUzOnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"meeting_sample.mp3\")\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1pAuDMSz64t",
        "outputId": "845d5e18-a0f5-4b31-9b2c-9690ccf58361"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Everybody's talking about artificial intelligence these days, AI. Machine learning is another hot topic. Are they the same thing or are they different? And if so, what are those differences? And deep learning is another one that comes into play. I actually did a video on these three, artificial intelligence, machine learning and deep learning, and talked about where they fit. And there were a lot of comments on that, and I read those comments, and I'd like to address some of the most frequently asked questions so that we can clear up some of the myths and misconceptions around this. In addition, something else has happened since that video was recorded. And that is the absolute explosion of this area of generative AI. Things like large language models and chatbots has seemed to be taking over the world. We see them everywhere. Really interesting technology. And then also things like deep fakes. These are all within the realm of AI, but how do they fit within each other? How are they related to each other? We're going to take a look at that in this video and try to explain how all these technologies relate and how we can use them. First off, a little bit of a disclaimer. I'm going to have to simplify some of these concepts in order to not make this video last for a week. So those of you that are really deep experts in the field apologies in advance. But we're going to try to make this simple and that will involve some generalizations. First of all, to start with AI. Artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence. What is intelligence? Well, it could be a lot of different things, but generally we tend to think of it as the ability to learn, to infer, and to reason, things like that. So that's what we're trying to do in the broad field of AI, of artificial intelligence. And if we look at a timeline of AI, it really kind of started back around this time frame. And in those days, it was very premature. Most people had not even heard of it. And it basically was a research project. But I can tell you, as an undergrad, which for me was back during these times, we were doing AI work. In fact, we would use programming languages like Lisp or ProLog. And these kinds of things were kind of the predecessors to what became later expert systems. And this was a technology. Again, some of these things existed previous, but that's when it really hit the kind of a critical mass and became more popularized. So expert systems of the 1980s, maybe in the 90s. And again, we used technologies like this. All of this was something that we did before we ever touched in to the next topic I'm going to talk about. And that's the area of machine learning. Machine learning is, as its name implies, the machine is learning. I don't have to program it. I give it lots of information and it observes things. So for instance, if I start doing this, if I give you this and then ask you to predict what's the next thing that's going to be there, well, you might get it, you might not. And you have very limited training data to base this on. But if I gave you one of those and then ask you what to predict, would you happen next, well, you're probably going to say this. And then you're going to say it's this. And then you think you got it all figured out. And then you see one of these. And then all of a sudden, I give you one of those and throw you a curveball. So this, in fact, and then maybe it goes on like this. So a machine learning algorithm is really good at looking at patterns and discovering patterns within data. The more training data you can give it, the more confident it can be in predicting. So predictions are one of the things that machine learning is particularly good at. Another thing is spotting outliers like this and saying, oh, that doesn't belong in, it looks different than all the other stuff because the sequence was broken. So that's particularly useful in cybersecurity, the area that I work in, because we're looking for outliers. We're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do. So this technology machine learning is particularly useful for us. And machine learning really came along and became more popularized in this time frame in the 2010s. And again, back when I was an undergrad riding my dinosaur to class, we were doing this kind of stuff. We never once talked about machine learning. It might have existed, but it really wasn't hadn't hit the popular mindset yet. But this technology has matured greatly over the last few decades, and now it becomes the basis of a lot we do going forward. The next layer of our Venn diagram involves deep learning. Well, it's deep learning in the sense that with deep learning, we use these things called neural networks. Neural networks are ways that in a computer, we simulate and mimic the way the human brain works, at least to the extent that we understand how the brain works. And it's called deep because we have multiple layers of those neural networks. And the interesting thing about these is they will simulate the way a brain operates. But I don't know if you've noticed, but human brains can be a little bit unpredictable. You put certain things in, you don't always get the very same thing out. And deep learning is the same way. In some cases, we're not actually able to fully understand why we get the results we do because there are so many layers to the neural network. It's a little bit hard to decompose and figure out exactly what's in there. But this has become a very important part, and a very important advancement. That also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI. The most recent advancements in the field of artificial intelligence all really are in this space, the area of generative AI. Now, I'm going to introduce a term that you may not be familiar with. It's the idea of foundation models. Foundation models is where we get some of these kinds of things. For instance, an example of a foundation model would be a large language model, which is where we take language and we model it. And we make predictions in this technology where if I see certain types of words, then I can sort of predict what the next set of words will be. I'm going to oversimplify here for the sake of simplicity. But think about this as a little bit like the autocomplete. When you start typing something in and then it predicts what your next word will be, except in this case with large language models, they're not predicting the next word. They're predicting the next sentence, the next paragraph, the next entire document. So there's a really an amazing exponential leap in what these things are able to do. And we call all of these technologies generative because they are generating new content. Some people have actually made the argument that the generative AI isn't really generative that these technologies are really just regurgitating existing information and putting it in different format. Well, let me give you an analogy. If you take music, for instance, then every note has already been invented. So in a sense, every song is just a recombination, some other permutation of all the notes that already exist already, and just putting them in a different order. Well, we don't say new music doesn't exist. People are still composing and creating new songs from the existing information. I'm going to say, Gen AI is similar. It's an analogy, so there'll be some imperfections in it, but you get the general idea. Actually, new content can be generated out of these. And there are a lot of different forms that this can take. Other types of models are audio models, video models, and things like that. Well, in fact, these we can use to create deepfakes. And deepfakes are examples where we're able to take, for instance, a person's voice and recreate that. And then habits seem like the person said things they never said. Well, it's really useful in entertainment situations, in parodies and things like that, or if someone's losing their voice. Then you could capture their voice, and then they'd be able to type, and you'd be able to hear it in their voice. But there's also a lot of cases where this stuff could be abused. The chat bots, again, come from this space. The deepfakes come from this space. But they're all part of generative AI and all part of these foundation models. And this, again, is the area that has really caused all of us to really pay attention to AI. The possibilities of generating new content, or in some cases, summarizing existing content, and giving us something that is bite size and manageable. This is what has gotten all of the attention. This is where the chat bots and all of these things come in. In the early days, AI's adoption started off pretty slowly. Most people didn't even know it existed. And if they did, it was something that always seemed like it was about five to 10 years away. But then machine learning, deep learning, and things like that came along. And we started seeing some uptick. Then foundation models, Gen AI, and the light came along. And this stuff went straight to the moon. These foundation models are what have changed the adoption curve. And now you see AI being adopted everywhere. And the thing for us to understand is where this is, where it fits in, and make sure that we can reap the benefits from all of this technology. If you like this video and want to see more like it, please like and subscribe. If you have any questions or want to share your thoughts about this topic, please leave a comment below. I, I, my book, and the rest of Tologies and, and those videos, in interviews or thing like, show, and these parts gave us use them. And it started, very slowly move us to the besteht zone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"meeting_sample.mp3\")\n",
        "transcript = result[\"text\"]\n",
        "\n",
        "# Split transcript into paragraphs (heuristic: split by sentence-ending punctuation)\n",
        "import re\n",
        "paragraphs = re.split(r'(?<=[.!?])\\s+', transcript)\n",
        "\n",
        "# Print each paragraph nicely\n",
        "for para in paragraphs:\n",
        "    print(para)\n",
        "    print()  # extra line for spacing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXu4JEdv3XDf",
        "outputId": "619bbeb7-70e3-4584-ec98-bf109a719c13"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Everybody's talking about artificial intelligence these days, AI.\n",
            "\n",
            "Machine learning is another hot topic.\n",
            "\n",
            "Are they the same thing or are they different?\n",
            "\n",
            "And if so, what are those differences?\n",
            "\n",
            "And deep learning is another one that comes into play.\n",
            "\n",
            "I actually did a video on these three, artificial intelligence, machine learning and deep learning, and talked about where they fit.\n",
            "\n",
            "And there were a lot of comments on that, and I read those comments, and I'd like to address some of the most frequently asked questions so that we can clear up some of the myths and misconceptions around this.\n",
            "\n",
            "In addition, something else has happened since that video was recorded.\n",
            "\n",
            "And that is the absolute explosion of this area of generative AI.\n",
            "\n",
            "Things like large language models and chatbots has seemed to be taking over the world.\n",
            "\n",
            "We see them everywhere.\n",
            "\n",
            "Really interesting technology.\n",
            "\n",
            "And then also things like deep fakes.\n",
            "\n",
            "These are all within the realm of AI, but how do they fit within each other?\n",
            "\n",
            "How are they related to each other?\n",
            "\n",
            "We're going to take a look at that in this video and try to explain how all these technologies relate and how we can use them.\n",
            "\n",
            "First off, a little bit of a disclaimer.\n",
            "\n",
            "I'm going to have to simplify some of these concepts in order to not make this video last for a week.\n",
            "\n",
            "So those of you that are really deep experts in the field apologies in advance.\n",
            "\n",
            "But we're going to try to make this simple and that will involve some generalizations.\n",
            "\n",
            "First of all, to start with AI.\n",
            "\n",
            "Artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence.\n",
            "\n",
            "What is intelligence?\n",
            "\n",
            "Well, it could be a lot of different things, but generally we tend to think of it as the ability to learn, to infer, and to reason, things like that.\n",
            "\n",
            "So that's what we're trying to do in the broad field of AI, of artificial intelligence.\n",
            "\n",
            "And if we look at a timeline of AI, it really kind of started back around this time frame.\n",
            "\n",
            "And in those days, it was very premature.\n",
            "\n",
            "Most people had not even heard of it.\n",
            "\n",
            "And it basically was a research project.\n",
            "\n",
            "But I can tell you, as an undergrad, which for me was back during these times, we were doing AI work.\n",
            "\n",
            "In fact, we would use programming languages like Lisp or ProLog.\n",
            "\n",
            "And these kinds of things were kind of the predecessors to what became later expert systems.\n",
            "\n",
            "And this was a technology.\n",
            "\n",
            "Again, some of these things existed previous, but that's when it really hit the kind of a critical mass and became more popularized.\n",
            "\n",
            "So expert systems of the 1980s, maybe in the 90s.\n",
            "\n",
            "And again, we used technologies like this.\n",
            "\n",
            "All of this was something that we did before we ever touched in to the next topic I'm going to talk about.\n",
            "\n",
            "And that's the area of machine learning.\n",
            "\n",
            "Machine learning is, as its name implies, the machine is learning.\n",
            "\n",
            "I don't have to program it.\n",
            "\n",
            "I give it lots of information and it observes things.\n",
            "\n",
            "So for instance, if I start doing this, if I give you this and then ask you to predict what's the next thing that's going to be there, well, you might get it, you might not.\n",
            "\n",
            "And you have very limited training data to base this on.\n",
            "\n",
            "But if I gave you one of those and then ask you what to predict, would you happen next, well, you're probably going to say this.\n",
            "\n",
            "And then you're going to say it's this.\n",
            "\n",
            "And then you think you got it all figured out.\n",
            "\n",
            "And then you see one of these.\n",
            "\n",
            "And then all of a sudden, I give you one of those and throw you a curveball.\n",
            "\n",
            "So this, in fact, and then maybe it goes on like this.\n",
            "\n",
            "So a machine learning algorithm is really good at looking at patterns and discovering patterns within data.\n",
            "\n",
            "The more training data you can give it, the more confident it can be in predicting.\n",
            "\n",
            "So predictions are one of the things that machine learning is particularly good at.\n",
            "\n",
            "Another thing is spotting outliers like this and saying, oh, that doesn't belong in, it looks different than all the other stuff because the sequence was broken.\n",
            "\n",
            "So that's particularly useful in cybersecurity, the area that I work in, because we're looking for outliers.\n",
            "\n",
            "We're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do.\n",
            "\n",
            "So this technology machine learning is particularly useful for us.\n",
            "\n",
            "And machine learning really came along and became more popularized in this time frame in the 2010s.\n",
            "\n",
            "And again, back when I was an undergrad riding my dinosaur to class, we were doing this kind of stuff.\n",
            "\n",
            "We never once talked about machine learning.\n",
            "\n",
            "It might have existed, but it really wasn't hadn't hit the popular mindset yet.\n",
            "\n",
            "But this technology has matured greatly over the last few decades, and now it becomes the basis of a lot we do going forward.\n",
            "\n",
            "The next layer of our Venn diagram involves deep learning.\n",
            "\n",
            "Well, it's deep learning in the sense that with deep learning, we use these things called neural networks.\n",
            "\n",
            "Neural networks are ways that in a computer, we simulate and mimic the way the human brain works, at least to the extent that we understand how the brain works.\n",
            "\n",
            "And it's called deep because we have multiple layers of those neural networks.\n",
            "\n",
            "And the interesting thing about these is they will simulate the way a brain operates.\n",
            "\n",
            "But I don't know if you've noticed, but human brains can be a little bit unpredictable.\n",
            "\n",
            "You put certain things in, you don't always get the very same thing out.\n",
            "\n",
            "And deep learning is the same way.\n",
            "\n",
            "In some cases, we're not actually able to fully understand why we get the results we do because there are so many layers to the neural network.\n",
            "\n",
            "It's a little bit hard to decompose and figure out exactly what's in there.\n",
            "\n",
            "But this has become a very important part, and a very important advancement.\n",
            "\n",
            "That also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI.\n",
            "\n",
            "The most recent advancements in the field of artificial intelligence all really are in this space, the area of generative AI.\n",
            "\n",
            "Now, I'm going to introduce a term that you may not be familiar with.\n",
            "\n",
            "It's the idea of foundation models.\n",
            "\n",
            "Foundation models is where we get some of these kinds of things.\n",
            "\n",
            "For instance, an example of a foundation model would be a large language model, which is where we take language and we model it.\n",
            "\n",
            "And we make predictions in this technology where if I see certain types of words, then I can sort of predict what the next set of words will be.\n",
            "\n",
            "I'm going to oversimplify here for the sake of simplicity.\n",
            "\n",
            "But think about this as a little bit like the autocomplete.\n",
            "\n",
            "When you start typing something in and then it predicts what your next word will be, except in this case with large language models, they're not predicting the next word.\n",
            "\n",
            "They're predicting the next sentence, the next paragraph, the next entire document.\n",
            "\n",
            "So there's a really an amazing exponential leap in what these things are able to do.\n",
            "\n",
            "And we call all of these technologies generative because they are generating new content.\n",
            "\n",
            "Some people have actually made the argument that the generative AI isn't really generative that these technologies are really just regurgitating existing information and putting it in different format.\n",
            "\n",
            "Well, let me give you an analogy.\n",
            "\n",
            "If you take music, for instance, then every note has already been invented.\n",
            "\n",
            "So in a sense, every song is just a recombination, some other permutation of all the notes that already exist already, and just putting them in a different order.\n",
            "\n",
            "Well, we don't say new music doesn't exist.\n",
            "\n",
            "People are still composing and creating new songs from the existing information.\n",
            "\n",
            "I'm going to say, Gen AI is similar.\n",
            "\n",
            "It's an analogy, so there'll be some imperfections in it, but you get the general idea.\n",
            "\n",
            "Actually, new content can be generated out of these.\n",
            "\n",
            "And there are a lot of different forms that this can take.\n",
            "\n",
            "Other types of models are audio models, video models, and things like that.\n",
            "\n",
            "Well, in fact, these we can use to create deepfakes.\n",
            "\n",
            "And deepfakes are examples where we're able to take, for instance, a person's voice and recreate that.\n",
            "\n",
            "And then habits seem like the person said things they never said.\n",
            "\n",
            "Well, it's really useful in entertainment situations, in parodies and things like that, or if someone's losing their voice.\n",
            "\n",
            "Then you could capture their voice, and then they'd be able to type, and you'd be able to hear it in their voice.\n",
            "\n",
            "But there's also a lot of cases where this stuff could be abused.\n",
            "\n",
            "The chat bots, again, come from this space.\n",
            "\n",
            "The deepfakes come from this space.\n",
            "\n",
            "But they're all part of generative AI and all part of these foundation models.\n",
            "\n",
            "And this, again, is the area that has really caused all of us to really pay attention to AI.\n",
            "\n",
            "The possibilities of generating new content, or in some cases, summarizing existing content, and giving us something that is bite size and manageable.\n",
            "\n",
            "This is what has gotten all of the attention.\n",
            "\n",
            "This is where the chat bots and all of these things come in.\n",
            "\n",
            "In the early days, AI's adoption started off pretty slowly.\n",
            "\n",
            "Most people didn't even know it existed.\n",
            "\n",
            "And if they did, it was something that always seemed like it was about five to 10 years away.\n",
            "\n",
            "But then machine learning, deep learning, and things like that came along.\n",
            "\n",
            "And we started seeing some uptick.\n",
            "\n",
            "Then foundation models, Gen AI, and the light came along.\n",
            "\n",
            "And this stuff went straight to the moon.\n",
            "\n",
            "These foundation models are what have changed the adoption curve.\n",
            "\n",
            "And now you see AI being adopted everywhere.\n",
            "\n",
            "And the thing for us to understand is where this is, where it fits in, and make sure that we can reap the benefits from all of this technology.\n",
            "\n",
            "If you like this video and want to see more like it, please like and subscribe.\n",
            "\n",
            "If you have any questions or want to share your thoughts about this topic, please leave a comment below.\n",
            "\n",
            "You\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdxPa99v3zFa",
        "outputId": "20ad838f-ac1f-445e-d751-cbeb2559387f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
        "\n",
        "# Prepare the text\n",
        "text = result[\"text\"]  # transcript from Whisper\n",
        "\n",
        "# Hugging Face models work best on inputs under 512 tokens.\n",
        "# So if text is long, split it into chunks.\n",
        "max_chunk_length = 500  # words\n",
        "\n",
        "words = text.split()\n",
        "chunks = [' '.join(words[i:i+max_chunk_length]) for i in range(0, len(words), max_chunk_length)]\n",
        "\n",
        "# Generate summaries for each chunk\n",
        "summaries = []\n",
        "for chunk in chunks:\n",
        "    input_text = \"summarize: \" + chunk\n",
        "    summary = summarizer(input_text, max_length=120, min_length=30, do_sample=False)\n",
        "    summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "# Combine and print the summaries\n",
        "final_summary = \"\\n\\n\".join(summaries)\n",
        "print(\" Summary:\\n\")\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH-BOr1L31-U",
        "outputId": "de1fbbd3-e359-4f53-9a04-7a88609c6231"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Summary:\n",
            "\n",
            "we're going to try to explain how all these technologies relate and how we can use them . if we look at a timeline of AI, it really started back around this time frame . we used technology like this before we ever touched in to the next topic .\n",
            "\n",
            "a machine learning algorithm is really good at looking at patterns and discovering patterns within data . the more training data you can give it, the more confident it can be in predicting . machine learning is particularly useful in cybersecurity, the area that I work in .\n",
            "\n",
            "foundation models are where we get some of these kinds of things . they're predicting what your next word will be, except for large language models . generative AI isn't really generative because they are regurgitating existing information .\n",
            "\n",
            "this is where the chat bots and all of these things come in . this is the area that has really caused all of us to really pay attention to AI . in the early days, AI's adoption started off pretty slowly .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqhxHO5z4upI",
        "outputId": "73e2405c-7d5f-4b18-971a-bdb5dd2469cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_auto_page_break(auto=True, margin=15)\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "# Add line by line\n",
        "for line in final_summary.split('\\n'):\n",
        "    pdf.multi_cell(0, 10, line)\n",
        "\n",
        "pdf.output(\"summary.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2fMYOzEA42qK",
        "outputId": "92c3135d-45d0-46ef-e5dd-dd5786c1d442"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"summary.pdf\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5yA_zay15G6B",
        "outputId": "a3dcd68e-8410-4631-e466-7eba98b5fe72"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_52b972c1-06ef-4f10-a87d-c5681e40d5d6\", \"summary.pdf\", 1576)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8gzNR7952UGI"
      }
    }
  ]
}